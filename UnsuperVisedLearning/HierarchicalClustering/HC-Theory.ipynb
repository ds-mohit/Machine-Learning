{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a34100",
   "metadata": {},
   "source": [
    "# ```Hierarchical Clustering```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c336d84",
   "metadata": {},
   "source": [
    "```Hierarchical Clustering``` is an unsupervised learning algorithm used to group similar data points into clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe1ab87",
   "metadata": {},
   "source": [
    " ```Hierarchical clustering``` creates a tree-like structure (dendrogram) that shows how data points are grouped together at different levels of similarity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41b7b51",
   "metadata": {},
   "source": [
    "```üîç Types of Hierarchical Clustering```\n",
    "There are two main approaches:\n",
    "\n",
    "```1. Agglomerative (Bottom-Up)``` ‚Äì Most common\n",
    "\n",
    "Starts with each data point as its own cluster\n",
    "\n",
    "Iteratively merges the closest pairs of clusters\n",
    "\n",
    "Continues until all data points are in a single cluster or a desired number is reached\n",
    "\n",
    "```2. Divisive (Top-Down)``` ‚Äì Less common\n",
    "\n",
    "Starts with one single cluster containing all data points\n",
    "\n",
    "Recursively splits clusters into smaller clusters\n",
    "\n",
    "Continues until each data point is its own cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57911e15",
   "metadata": {},
   "source": [
    "```üß† How It Works (Agglomerative Approach) : ```\n",
    "```1. Start with each data point as its own cluster :```   \n",
    "If there are n data points, you start with n clusters (each point is a cluster).\n",
    "\n",
    "```2. Compute the distance (or similarity) between all clusters :```  \n",
    "Use a distance metric like:\n",
    "\n",
    "Euclidean distance\n",
    "\n",
    "Manhattan distance\n",
    "\n",
    "Cosine similarity\n",
    "\n",
    "And a linkage method to calculate distance between clusters:\n",
    "\n",
    "Single linkage: Minimum pairwise distance\n",
    "\n",
    "Complete linkage: Maximum pairwise distance\n",
    "\n",
    "Average linkage: Average pairwise distance\n",
    "\n",
    "Ward's linkage: Minimizes total within-cluster variance\n",
    "\n",
    "```3. Merge the two closest clusters``` :   \n",
    "Find the two clusters with the smallest distance and merge them into one cluster.\n",
    "\n",
    "```4. Update the distance matrix``` :  \n",
    "Recalculate the distances between the new cluster and all the remaining clusters using the chosen linkage method.\n",
    "\n",
    "```5. Repeat steps 3 and 4 until one cluster remains :  ```\n",
    "Continue merging clusters until:\n",
    "\n",
    "All data points are merged into one big cluster, or\n",
    "\n",
    "You reach a desired number of clusters.\n",
    "\n",
    "```6. Plot the dendrogram (optional but useful) : ```\n",
    "A dendrogram is a tree-like diagram that shows the order of merges and the distance at which merges occurred.\n",
    "\n",
    "By cutting the dendrogram at a certain height, you can choose the number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f40969c",
   "metadata": {},
   "source": [
    "```‚úÖ Steps of Divisive Hierarchical Clustering:```\n",
    "\n",
    "```1. Start with all data in one cluster :```\n",
    "Treat the entire dataset as one single cluster.\n",
    "\n",
    "```2. Choose a cluster to split : ```\n",
    "Select the cluster that shows the most internal dissimilarity (e.g., has the highest variance).\n",
    "\n",
    "``` 3. Split the chosen cluster :```\n",
    "Use a flat clustering algorithm (e.g., K-means with K=2) to divide it into two sub-clusters.\n",
    "\n",
    "``` 4. Evaluate the split : ```\n",
    "Assess whether the split improves the clustering quality using internal metrics like silhouette score.\n",
    "\n",
    "``` 5. Repeat recursively :```\n",
    "Continue splitting the resulting sub-clusters until:\n",
    "\n",
    "Each cluster has only one data point or\n",
    "\n",
    "A stopping condition is met (e.g., number of clusters reached, or minimal improvement)\n",
    "\n",
    "```6. Build a dendrogram : ```\n",
    "The process can be visualized in a tree-like diagram, where the root is the entire dataset, and leaves are individual data points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022e7b69",
   "metadata": {},
   "source": [
    "```üå≥ Dendrogram :```\n",
    "\n",
    "A ```dendrogram``` is a tree-like diagram that shows the merging of clusters. By cutting the dendrogram at a certain height, you can choose the number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498163ce",
   "metadata": {},
   "source": [
    "### for example:\n",
    "üìä Dataset\n",
    "We have 4 points in 2D space:\n",
    "\n",
    "Point\tCoordinates\n",
    "A\t(1, 2)\n",
    "\n",
    "B\t(2, 2)\n",
    "\n",
    "C\t(5, 5)\n",
    "\n",
    "D\t(6, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def58f7b",
   "metadata": {},
   "source": [
    "<img src=\"example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f34abd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
